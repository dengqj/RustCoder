# Rust Coder

API and MCP services that generate fully functional Rust projects from natural language descriptions. These services leverage LLMs to create complete Rust cargo projects, compile Rust source code, and automatically fix compiler errors.

---

## âœ¨ Features

- **Generate Rust Projects** ğŸ“¦ - Transform text descriptions into complete Rust projects.
- **Automatic Compilation & Fixing** ğŸ›  - Detect and resolve errors during compilation.
- **Vector Search** ğŸ” - Search for similar projects and errors.
- **Docker Containerization** ğŸ³ - Easy deployment with Docker.
- **Asynchronous Processing** â³ - Handle long-running operations efficiently.

---

## ğŸ“‹ Prerequisites

Ensure you have the following installed:

- **Docker & Docker Compose** ğŸ³

Or, if you want to run the services directly on your own computer:

- **Python 3.8+** ğŸ
- **Rust Compiler and cargo tools** ğŸ¦€ 

---

## ğŸ“¦ Install

```bash
git clone <repository-url>
cd Rust_coder_lfx
```

## ğŸš€ Configure and run

### Using Docker (Recommended)

Edit the `docker-compose.yml` file and specify your own LLM API server. The default config assumes that you have a [Gaia node like this](https://github.com/GaiaNet-AI/node-configs/tree/main/qwen-2.5-coder-3b-instruct-gte) running on `localhost` port `8080`. The alternative configuration shown below uses a [public Gaia node for coding assistance](https://docs.gaianet.ai/nodes#coding-assistant-agents).

```
- LLM_API_BASE=https://coder.gaia.domains/v1
- LLM_MODEL=Qwen2.5-Coder-32B-Instruct-Q5_K_M
- LLM_EMBED_MODEL=nomic-embed
- LLM_API_KEY=<YOUR API KEY FROM GAIANET.AI>
- LLM_EMBED_SIZE=768
```

Start the services.

```bash
docker-compose up -d
```

Stop the services.

```bash
docker-compose stop
```

### Manual Setup

By default, you will need a [Qdrant server](https://qdrant.tech/documentation/quickstart/) running on `localhost` port `6333`. You also need a [local Gaia node](https://github.com/GaiaNet-AI/node-configs/tree/main/qwen-2.5-coder-3b-instruct-gte). Set the following environment variables in your terminal to point to the Qdrant and Gaia instances, as well as your Rust compiler tools.

```
QDRANT_HOST
QDRANT_PORT
LLM_API_BASE
LLM_MODEL
LLM_EMBED_MODEL
LLM_API_KEY
LLM_EMBED_SIZE
CARGO_PATH
RUST_COMPILER_PATH
```

Start the services.

```bash
pip install -r requirements.txt
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

---

## ğŸ”¥ Usage

The API provides the following endpoints:

### ğŸ¯ Generate a Project

**Endpoint:** `POST /generate-sync`

**Example:**

```bash
  curl -X POST http://localhost:8000/generate-sync \
  -H "Content-Type: application/json" \
  -d '{"description": "A command-line calculator in Rust", "requirements": "Should support addition, subtraction, multiplication, and division"}'
```

#### ğŸ“¥ Request Body:

```
{
  "description": "A command-line calculator in Rust",
  "requirements": "Should support addition, subtraction, multiplication, and division"
}
```

#### ğŸ“¤ Response:

```
[filename: Cargo.toml]
... ...

[filename: src/main.rs]
... ...
```

### ğŸ›  Compile a Project

**Endpoint:** `POST /compile`

**Example:**

```bash
curl -X POST http://localhost:8000/compile \
-H "Content-Type: application/json" \
-d '{
  "code": "[filename: Cargo.toml]\n[package]\nname = \"hello_world\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\n\n[filename: src/main.rs]\nfn main() {\n    println!(\"Hello, World!\");\n}"
}'
```

#### ğŸ“¥ Request Body:

```
{
  "code": "[filename: Cargo.toml]\n[package]\nname = \"hello_world\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\n\n[filename: src/main.rs]\nfn main() {\n    println!(\"Hello, World!\");\n}"
}
```

#### ğŸ“¤ Response:

```
{
  "success": true,
  "files": [
    "Cargo.toml",
    "src/main.rs"
  ],
  "build_output": "Build successful",
  "run_output": "Hello, World!\n"
}
```

### ğŸ©¹ Compile and fix errors 

**Endpoint:** `POST /compile-and-fix`

**Example:**

```bash
curl -X POST http://localhost:8000/compile-and-fix \
-H "Content-Type: application/json" \
-d '{
  "code": "[filename: Cargo.toml]\n[package]\nname = \"hello_world\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\n\n[filename: src/main.rs]\nfn main() {\n    println!(\"Hello, World!\" // Missing closing parenthesis\n}",
  "description": "A simple hello world program",
  "max_attempts": 3
}'
```

#### ğŸ“¥ Request Body:

```
{
  "code": "[filename: Cargo.toml]\n[package]\nname = \"hello_world\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\n\n[filename: src/main.rs]\nfn main() {\n    println!(\"Hello, World!\" // Missing closing parenthesis\n}",
  "description": "A simple hello world program",
  "max_attempts": 3
}
```
     
#### ğŸ“¤ Response:

```
{
  "success": true,
  "attempts": [
    {
      "attempt": 1,
      "success": false,
      "output": "   Compiling hello_world v0.1.0 (/tmp/tmpk_0n65md)\nerror: mismatched closing delimiter: `}`\n --> src/main.rs:2:13\n  |\n1 | fn main() {\n  |           - closing delimiter possibly meant for this\n2 |     println!(\"Hello, World!\" // Missing closing parenthesis\n  |             ^ unclosed delimiter\n3 | }\n  | ^ mismatched closing delimiter\n\nerror: could not compile `hello_world` (bin \"hello_world\") due to 1 previous error\n"
    },
    {
      "attempt": 2,
      "success": true,
      "output": null
    }
  ],
  "final_files": {
    "Cargo.toml": "[package]\nname = \"hello_world\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]",
    "src/main.rs": "fn main() {\n    println!(\"Hello, World!\");\n}"
  },
  "build_output": "Build successful",
  "run_output": "Hello, World!\n",
  "similar_project_used": false,
  "combined_text": "[filename: Cargo.toml]\n[package]\nname = \"hello_world\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\n\n[filename: src/main.rs]\nfn main() {\n    println!(\"Hello, World!\");\n}"
}
```

### ğŸ¯ Generate a Project Async

**Endpoint:** `POST /generate`

**Example:**

```bash
  curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"description": "A command-line calculator in Rust", "requirements": "Should support addition, subtraction, multiplication, and division"}'
```

#### ğŸ“¥ Request Body:

```json
{
  "description": "A command-line calculator in Rust",
  "requirements": "Should support addition, subtraction, multiplication, and division"
}
```

#### ğŸ“¤ Response:

```json
{
  "project_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "processing",
  "message": "Project generation started"
}
```

### ğŸ“Œ Check Project Status

**Endpoint:** `GET /project/{project_id}`

**Example:**

```bash
curl http://localhost:8000/project/123e4567-e89b-12d3-a456-426614174000
```

#### ğŸ“¤ Response:

```json
{
  "project_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "completed",
  "message": "Project generated successfully",
  "files": ["Cargo.toml", "src/main.rs", "README.md"],
  "build_output": "...",
  "run_output": "..."
}
```

### ğŸ“Œ Get Generated Files

**Endpoint:** `GET /project/{project_id}/files/path_to_file`

**Example:**

```bash
curl http://localhost:8000/project/123e4567-e89b-12d3-a456-426614174000/files/src/main.rs
```

#### ğŸ“¤ Response:

```
fn main() {
    ... ...
}
```

---

## ğŸ”§ MCP (Model-Compiler-Processor) tools

The MCP server is available via the HTTP SSE transport via the `http://localhost:3000/sse` URL. The MCP server can be accessed using the [cmcp command-line client](https://github.com/RussellLuo/cmcp). To install the `cmcp` tool,

```bash
pip install cmcp
```

### ğŸ¯ Generate a new project

**tool:** `generate`

#### ğŸ“¥ Request example:

```bash
cmcp http://localhost:3000 tools/call name=generate arguments:='{"description": "A command-line calculator in Rust", "requirements": "Should support addition, subtraction, multiplication, and division"}'
```

#### ğŸ“¤ Response:

```json
{
  "meta": null,
  "content": [
    {
      "type": "text",
      "text": "[filename: Cargo.toml] ... [filename: src/main.rs] ... ",
      "annotations": null
    }
  ],
  "isError": false
}
```
### ğŸ©¹ Compile and Fix Rust Code

**tool:** `compile_and_fix`

#### ğŸ“¥ Request example:

```bash
cmcp http://localhost:3000 tools/call name=compile_and_fix arguments:='{"code": "[filename: Cargo.toml]\n[package]\nname = \"hello_world\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\n\n[filename: src/main.rs]\nfn main() {\n    println!(\"Hello, World!\" // Missing closing parenthesis \n}" }'
```

#### ğŸ“¤ Response:

```json
{
  "meta": null,
  "content": [
    {
      "type": "text",
      "text": "[filename: Cargo.toml]\n[package]\nname = \"hello_world\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\n\n[filename: src/main.rs]\nfn main() {\n    println!(\"Hello, World!\"); \n}",
      "annotations": null
    }
  ],
  "isError": false
}
```

---

## ğŸ“‚ Project Structure

```
Rust_coder_lfx/
â”œâ”€â”€ app/                  # Application code
â”‚   â”œâ”€â”€ compiler.py       # Rust compilation handling
â”‚   â”œâ”€â”€ llm_client.py     # LLM API client
â”‚   â”œâ”€â”€ main.py           # FastAPI application
â”‚   â”œâ”€â”€ mcp_service.py    # Model-Compiler-Processor service
â”‚   â”œâ”€â”€ prompt_generator.py # LLM prompt generation
â”‚   â”œâ”€â”€ response_parser.py # Parse LLM responses into files
â”‚   â”œâ”€â”€ vector_store.py   # Vector database interface
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                 # Data storage
â”‚   â”œâ”€â”€ error_examples/   # Error examples for vector search
â”‚   â””â”€â”€ project_examples/ # Project examples for vector search
â”œâ”€â”€ docker-compose.yml    # Docker Compose configuration
â”œâ”€â”€ Dockerfile            # Docker configuration
â”œâ”€â”€ examples/             # Example scripts for using the API
â”œâ”€â”€ output/               # Generated project output
â”œâ”€â”€ parse_and_save_qna.py # Q&A parsing utility
â”œâ”€â”€ qdrant_data/          # Vector database storage
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ templates/            # API templates
```

---


## ğŸ§  How It Works

Vector Search: The system uses Qdrant for storing and searching project examples and error solutions.

LLM Integration: Communicates with LlamaEdge API for code generation and error fixing via `llm_client.py`.

Compilation Feedback Loop: Automatically compiles, detects errors, and fixes them using `compiler.py`.

File Parsing: Converts LLM responses into project files with `response_parser.py`.

---

## ğŸ¤ Contributing
Contributions are welcome! Feel free to submit a Pull Request. ğŸš€

---

## ğŸ“œ License
Licensed under [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html).
